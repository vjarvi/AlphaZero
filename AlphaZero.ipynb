{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA88RJ7vx6rcW1z7E/tjFe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from tqdm.notebook import trange\n",
        "import random\n",
        "import pickle\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "fczJc_4a0t-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93175ce7-ae56-4135-9d39-c62f22d9fc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gomoku():\n",
        "    \"\"\"\n",
        "    A class representing the gomoku game, played on a 15x15 board.\n",
        "    The objective is to place five of your marks in a horizontal, vertical,\n",
        "    or diagonal row. Default board size is 15x15.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rows=15, columns=15):\n",
        "      self.action_size = rows * columns\n",
        "      self.columns = columns\n",
        "      self.rows = rows\n",
        "      self.in_a_row = 5 # number of consecutive marks required to win\n",
        "\n",
        "    def __repr__(self):\n",
        "      return \"Gomoku\"\n",
        "\n",
        "    def get_initial_state(self):\n",
        "      return np.zeros((self.rows, self.columns))\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "      row = action // self.columns\n",
        "      column = action % self.columns\n",
        "      state[row, column] = player\n",
        "      return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "      return ((state == 0).astype(np.uint8))\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "      if action == None:\n",
        "        return False\n",
        "\n",
        "      row = action // self.columns\n",
        "      column = action % self.columns\n",
        "      player = state[row, column]\n",
        "\n",
        "      def count(offset_row, offset_column):\n",
        "        for i in range(1, self.in_a_row):\n",
        "          r = row + offset_row * i\n",
        "          c = column + offset_column * i\n",
        "          if (\n",
        "            r < 0\n",
        "            or r >= self.rows\n",
        "            or c < 0\n",
        "            or c >= self.columns\n",
        "            or state[r][c] != player\n",
        "            ):\n",
        "              return i - 1\n",
        "        return self.in_a_row - 1\n",
        "\n",
        "      return (\n",
        "        (count(1, 0) + count(-1, 0)) >= self.in_a_row - 1 # vertical\n",
        "        or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
        "        or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
        "        or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
        "      )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "      return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "      return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      if len(state.shape) == 3:\n",
        "        encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "      return encoded_state"
      ],
      "metadata": {
        "id": "tCiNWKRP_MyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectFour():\n",
        "    \"\"\"\n",
        "    Class representing ConnectFour game, 6x7 board.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rows=6, columns=7):\n",
        "      self.columns = 7\n",
        "      self.rows = 6\n",
        "      self.action_size = self.columns\n",
        "      self.in_a_row = 4 # number of consecutive marks required to win\n",
        "\n",
        "    def __repr__(self):\n",
        "      return \"ConnectFour\"\n",
        "\n",
        "    def get_initial_state(self):\n",
        "      return np.zeros((self.rows, self.columns))\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "      row = np.max(np.where(state[:, action] == 0))\n",
        "      state[row, action] = player\n",
        "      return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "      return ((state[0] == 0).astype(np.uint8))\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "      if action == None:\n",
        "          return False\n",
        "\n",
        "      row = np.min(np.where(state[:, action] != 0))\n",
        "      column = action\n",
        "      player = state[row][column]\n",
        "\n",
        "      def count(offset_row, offset_column):\n",
        "        for i in range(1, self.in_a_row):\n",
        "          r = row + offset_row * i\n",
        "          c = action + offset_column * i\n",
        "          if (\n",
        "            r < 0\n",
        "            or r >= self.rows\n",
        "            or c < 0\n",
        "            or c >= self.columns\n",
        "            or state[r][c] != player\n",
        "            ):\n",
        "              return i - 1\n",
        "        return self.in_a_row - 1\n",
        "\n",
        "      return (\n",
        "        count(1, 0) >= self.in_a_row - 1 # vertical\n",
        "        or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
        "        or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
        "        or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
        "      )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "      return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "      return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      if len(state.shape) == 3:\n",
        "        encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "\n",
        "      return encoded_state"
      ],
      "metadata": {
        "id": "qbrqzKT6Ip5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    Represents a node in a Monte Carlo Tree Search (MCTS) used for decision-making\n",
        "    in AlphaZero. Each node corresponds to a specific state of the game and contains information about its children, parent, visit counts and\n",
        "\n",
        "    Attributes:\n",
        "    - game: The game object, providing methods to compute game states and transitions.\n",
        "    - args: A dictionary of arguments/hyperparameters\n",
        "    - state: The current state of the game associated with this node.\n",
        "    - parent: The parent node of this node. None if it is the root.\n",
        "    - action_taken: The action that led to this node's state from the parent node.\n",
        "    - prior: The prior probability of selecting this node.\n",
        "    - children: A list of child nodes representing possible next states.\n",
        "    - visit_count: The number of times this node has been visited during MCTS simulations.\n",
        "    - value_sum: The cumulative value from simulations passing through this node.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.action_taken = action_taken\n",
        "        self.prior = prior\n",
        "\n",
        "        self.children = []\n",
        "\n",
        "        self.visit_count = visit_count\n",
        "        self.value_sum = 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def select(self):\n",
        "        best_child = None\n",
        "        best_ucb = -np.inf\n",
        "\n",
        "        for child in self.children:\n",
        "            ucb = self.get_ucb(child)\n",
        "            if ucb > best_ucb:\n",
        "                best_child = child\n",
        "                best_ucb = ucb\n",
        "\n",
        "        return best_child\n",
        "\n",
        "    def get_ucb(self, child):\n",
        "        if child.visit_count == 0:\n",
        "            q_value = 0\n",
        "        else:\n",
        "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "    def expand(self, policy):\n",
        "        for action, prob in enumerate(policy):\n",
        "            if prob > 0:\n",
        "                child_state = self.state.copy()\n",
        "                child_state = self.game.get_next_state(child_state, action, 1)\n",
        "                child_state = self.game.change_perspective(child_state, player=-1)\n",
        "\n",
        "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
        "                self.children.append(child)\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        self.value_sum += value\n",
        "        self.visit_count += 1\n",
        "\n",
        "        if self.parent is not None:\n",
        "            value = self.game.get_opponent_value(value)\n",
        "            self.parent.backpropagate(value)"
      ],
      "metadata": {
        "id": "_8WzierEYZ5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZeroParallel:\n",
        "    \"\"\"\n",
        "    Runs AlphaZero algorithm in parallel.\n",
        "\n",
        "    Attributes:\n",
        "    - model: The neural network model used to predict policy and value outputs.\n",
        "    - optimizer: The optimizer used for updating model weights.\n",
        "    - policy_loss_fn: Loss function for the policy head of the model.\n",
        "    - value_loss_fn: Loss function for the value head of the model.\n",
        "    - game: An instance of the game class defining game mechanics and rules.\n",
        "    - args: A dictionary of arguments/hyperparameters for training and MCTS.\n",
        "    - verbose: A boolean for showning model accuracy.\n",
        "    - mcts: An instance of the MCTSParallel class used for simulating moves.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, policy_loss_fn, value_loss_fn, game, args, verbose=True):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.policy_loss_fn = policy_loss_fn\n",
        "        self.value_loss_fn = value_loss_fn\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.verbose = verbose\n",
        "        self.mcts = MCTSParallel(game, args, model)\n",
        "\n",
        "    def get_canonical_boards(self,state, action_probs, player):\n",
        "        \"\"\"\n",
        "        state: np.ndarray shape of (rows, columns)\n",
        "        action_probs: np.ndarray shape of (action_size)\n",
        "        player: int\n",
        "\n",
        "        returns:\n",
        "        list of tuples (state, action_probs, player)\n",
        "\n",
        "        Does data augmentation by flipping the state and action_probs.\n",
        "        \"\"\"\n",
        "        game_name = repr(game)\n",
        "\n",
        "        # gomoku allows for more configurations of the board than connect four.\n",
        "        if game_name == \"Gomoku\":\n",
        "            memory = []\n",
        "            current_state = state.astype(np.int8)\n",
        "            current_probs = action_probs.astype(np.float32)\n",
        "\n",
        "            memory.extend([\n",
        "                [current_state, current_probs, player],\n",
        "                [np.flip(current_state, axis=0),\n",
        "                np.flip(current_probs.reshape(15, 15), axis=0).reshape(225),\n",
        "                player]\n",
        "            ])\n",
        "\n",
        "            for i in range(3):\n",
        "                current_state = np.rot90(current_state)\n",
        "                current_probs = current_probs.reshape(15, 15)\n",
        "                current_probs = np.rot90(current_probs)\n",
        "                current_probs = current_probs.reshape(225)\n",
        "\n",
        "                # Create flipped versions\n",
        "                memory.extend([\n",
        "                    [current_state, current_probs, player],\n",
        "                    [np.flip(current_state, axis=0),\n",
        "                    np.flip(current_probs.reshape(15, 15), axis=0).reshape(225),\n",
        "                    player]\n",
        "                ])\n",
        "\n",
        "        elif game_name == \"ConnectFour\":\n",
        "            memory = [\n",
        "              [state.astype(np.int8), action_probs.astype(np.float32), player],\n",
        "              [np.flip(state.astype(np.int8), axis=1),\n",
        "              np.flip(action_probs.astype(np.float32), axis=0),\n",
        "              player]\n",
        "            ]\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def selfPlay(self):\n",
        "        \"\"\"\n",
        "        returns:\n",
        "        list of tuples (state, action_probs, player)\n",
        "\n",
        "        Generates training data by simulating games using Monte Carlo Tree Search (MCTS).\n",
        "        \"\"\"\n",
        "\n",
        "        return_memory = []\n",
        "        player = 1\n",
        "        spGames = (SPG(self.game) for _ in range(self.args['num_parallel_games']))\n",
        "        active_games = list(spGames)\n",
        "\n",
        "        while active_games:\n",
        "\n",
        "            #monitor progress\n",
        "            print(f\"{len(active_games)} parallel games left\")\n",
        "\n",
        "            states = np.stack([spg.state for spg in active_games])\n",
        "            neutral_states = self.game.change_perspective(states, player)\n",
        "\n",
        "            self.mcts.search(neutral_states, active_games)\n",
        "\n",
        "            for i in range(len(active_games))[::-1]:\n",
        "                spg = active_games[i]\n",
        "\n",
        "                action_probs = np.zeros(self.game.action_size)\n",
        "                for child in spg.root.children:\n",
        "                    action_probs[child.action_taken] = child.visit_count\n",
        "                action_probs /= np.sum(action_probs)\n",
        "\n",
        "                spg.memory.extend((self.get_canonical_boards(spg.root.state, action_probs, player)))\n",
        "                #spg.memory.append((spg.root.state, action_probs, player)) (adding to memory without get_canonical_boards)\n",
        "\n",
        "                temperature_action_probs = (action_probs ** (1 / self.args['temperature']))\n",
        "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
        "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
        "\n",
        "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
        "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
        "\n",
        "                if is_terminal:\n",
        "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
        "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "                        return_memory.append((\n",
        "                            self.game.get_encoded_state(hist_neutral_state),\n",
        "                            hist_action_probs,\n",
        "                            hist_outcome\n",
        "                        ))\n",
        "                    del active_games[i]\n",
        "\n",
        "            player = self.game.get_opponent(player)\n",
        "\n",
        "        return return_memory\n",
        "\n",
        "    def train(self, memory):\n",
        "        \"\"\"\n",
        "        memory: list of tuples (state, action_probs, value)\n",
        "\n",
        "        training loop for neural network\n",
        "        \"\"\"\n",
        "\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "        random.shuffle(memory)\n",
        "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "            sample = memory[batchIdx:batchIdx+self.args['batch_size']]\n",
        "            state, policy_targets, value_targets = zip(*sample)\n",
        "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "            out_policy, out_value = self.model(state)\n",
        "\n",
        "            policy_loss = self.policy_loss_fn(out_policy, policy_targets)\n",
        "            value_loss = self.value_loss_fn(out_value, value_targets)\n",
        "            loss = args['policy_value_bias'] * policy_loss + (1 - args['policy_value_bias']) * value_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            policy_losses.append(policy_loss.item())\n",
        "            value_losses.append(value_loss.item())\n",
        "\n",
        "            # Visualize loss\n",
        "            if self.verbose and batchIdx % 50 == 0:\n",
        "                print(f\"batch {batchIdx // self.args['batch_size']}: policy_loss mean: {np.mean(policy_losses):.3f} | value loss mean: {np.mean(value_losses):.3f}\")\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Iterates between selfplay and model training\n",
        "        \"\"\"\n",
        "\n",
        "        for iteration in range(self.args['num_iterations']):\n",
        "            memory = []\n",
        "\n",
        "            # generate games\n",
        "            self.model.eval()\n",
        "            for selfPlay_iteration in range(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
        "                memory.extend(self.selfPlay())\n",
        "\n",
        "            if memory != []:\n",
        "              with open(f\"iteration_{iteration}_memory.plk\", \"wb\") as file:\n",
        "                pickle.dump(memory, file)\n",
        "\n",
        "            # train the model\n",
        "            self.model.train()\n",
        "            for epoch in range(self.args['num_epochs']):\n",
        "                self.train(memory)\n",
        "\n",
        "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
        "\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "\n",
        "class SPG:\n",
        "    \"\"\"\n",
        "    'SelfPlayGame' class for managing game states and trees during selfplay.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game):\n",
        "        self.state = game.get_initial_state()\n",
        "        self.memory = []\n",
        "        self.root = None\n",
        "        self.node = None"
      ],
      "metadata": {
        "id": "_59ZCiwCW3S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTSParallel:\n",
        "    \"\"\"\n",
        "    Performs Monte Carlo Tree Search (MCTS) in parallel.\n",
        "\n",
        "    Attributes:\n",
        "    - game: An instance of the game class defining game mechanics and rules.\n",
        "    - args: A dictionary of arguments/hyperparameters for training and MCTS.\n",
        "    - model: The neural network model used to predict policy and value outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, states, spGames):\n",
        "        \"\"\"\n",
        "        states: np.ndarray shape of (parallelgames, rows, columns)\n",
        "        spGames: list of SPG objects\n",
        "\n",
        "        Performs MCTS simulations in parallel.\n",
        "        \"\"\"\n",
        "\n",
        "        # get predictions (policy) and add noise\n",
        "        policy, _ = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
        "\n",
        "        # normalize and mask invalid moves.\n",
        "        game_name = repr(game)\n",
        "        if game_name == \"Gomoku\":\n",
        "          valid_moves = (states == 0).reshape(states.shape[0], -1).astype(np.uint8)\n",
        "        elif game_name == \"ConnectFour\":\n",
        "          valid_moves = (states[:, 0, :] == 0).astype(np.uint8)\n",
        "\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy, axis=1, keepdims=True)\n",
        "\n",
        "        # initialize root nodes\n",
        "        for i, spg in enumerate(spGames):\n",
        "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
        "            spg.root.expand(policy[i])\n",
        "\n",
        "        for search in range(self.args['num_mstc_searches']):\n",
        "            for spg in spGames:\n",
        "                spg.node = None\n",
        "                node = spg.root\n",
        "\n",
        "                while node.is_fully_expanded():\n",
        "                    node = node.select()\n",
        "\n",
        "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "                value = self.game.get_opponent_value(value)\n",
        "\n",
        "                if is_terminal:\n",
        "                    node.backpropagate(value)\n",
        "\n",
        "                else:\n",
        "                    spg.node = node\n",
        "\n",
        "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
        "\n",
        "            if expandable_spGames:\n",
        "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
        "\n",
        "                policy, value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
        "                )\n",
        "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "\n",
        "                if game_name == \"Gomoku\":\n",
        "                    valid_moves = (states == 0).reshape(states.shape[0], -1).astype(np.uint8)\n",
        "                elif game_name == \"ConnectFour\":\n",
        "                    valid_moves = (states[:, 0, :] == 0).astype(np.uint8)\n",
        "\n",
        "                policy *= valid_moves\n",
        "                policy /= np.sum(policy, axis=1, keepdims=True)\n",
        "\n",
        "                value = value.cpu().numpy()\n",
        "\n",
        "            for i, mappingIdx in enumerate(expandable_spGames):\n",
        "                node = spGames[mappingIdx].node\n",
        "                node.expand(policy[i])\n",
        "                node.backpropagate(value[i])"
      ],
      "metadata": {
        "id": "jhmkit0OXAYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Input: game state in shape (3, rows, columns). One dimension for player 1 pieces,\n",
        "         one for player 2 pieces, and one for empty spaces.\n",
        "  Output: policy in shape (action_size) and value in shape (1)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, game, num_resBlocks, num_hidden, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.startBlock = nn.Sequential(\n",
        "        nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(num_hidden),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # Stack of residual blocks\n",
        "    self.backBone = nn.ModuleList(\n",
        "        [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "    )\n",
        "\n",
        "    # Policy head outputs action probabilities\n",
        "    self.policyHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * game.rows * game.columns, game.action_size)\n",
        "    )\n",
        "\n",
        "    # Value head estimates game state value between [-1,1]\n",
        "    self.valueHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3 * game.rows * game.columns, 1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.startBlock(x)\n",
        "    for resBlock in self.backBone:\n",
        "      x = resBlock(x)\n",
        "    policy = self.policyHead(x)\n",
        "    value = self.valueHead(x)\n",
        "\n",
        "    return policy, value\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Residual block with two convolutional layers and skip connection\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_hidden):\n",
        "     super().__init__()\n",
        "     self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "     self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "     self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "     self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "     self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.bn2(self.conv2(x))\n",
        "    x += residual\n",
        "    x = self.relu(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "fj_ubxVd6dZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = Gomoku()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ResNet(game, num_resBlocks=9, num_hidden=128, device=device)\n",
        "#model.load_state_dict(torch.load(\"model_3_ConnectFour.pt\", map_location=device))\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "#optim.load_state_dict(torch.load(\"optimizer_3_ConnectFour.pt\", map_location=device))\n",
        "\n",
        "value_loss_fn = nn.MSELoss()\n",
        "policy_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# hyperparameters\n",
        "args = {\n",
        "    'C': 2,                          # UCB exploration constant\n",
        "    'num_mstc_searches': 1,        # Number of MCTS simulations per move\n",
        "    'num_iterations': 4,             # Training iterations\n",
        "    'num_selfPlay_iterations': 2,  # Self-play games per iteration\n",
        "    'num_parallel_games': 2,       # Number of games to play in parallel\n",
        "    'num_epochs': 4,                 # Training epochs per iteration\n",
        "    'batch_size': 128,              # Training batch size\n",
        "    'temperature': 1,                # Temperature for action selection\n",
        "    'policy_value_bias': 0.5,        # Balance between policy and value loss\n",
        "    'dirichlet_epsilon': 0.15,       # Exploration noise weight\n",
        "    'dirichlet_alpha': 0.15         # Dirichlet distribution parameter\n",
        "}\n",
        "\n",
        "alphaZero = AlphaZeroParallel(model, optim, policy_loss_fn, value_loss_fn, game, args)\n",
        "alphaZero.learn()"
      ],
      "metadata": {
        "id": "m8S7fw8hF3Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = ConnectFour()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 0.1,\n",
        "    'num_mcts_searches': 50,\n",
        "    'dirichlet_epsilon': 0.05,\n",
        "    'dirichlet_alpha': 0.1\n",
        "}\n",
        "\n",
        "model1 = ResNet(game, 9, 128, device)\n",
        "#model1.load_state_dict(torch.load(\"model_3_ConnectFour (1).pt\", map_location=device))\n",
        "model1.eval()\n",
        "\n",
        "model2 = ResNet(game, 9, 128, device)\n",
        "#model2.load_state_dict(torch.load(\"model_7_ConnectFour (1).pt\", map_location=device))\n",
        "model2.eval()\n",
        "\n",
        "mcts1 = MCTS(game, args, model1)\n",
        "mcts2 = MCTS(game, args, model2)\n",
        "\n",
        "state = game.get_initial_state()\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(state)\n",
        "\n",
        "    if player == 1:\n",
        "        neutral_state1 = game.change_perspective(state, player)\n",
        "        mcts_probs1, pos_value1, value1 = mcts1.search(neutral_state1)\n",
        "        action = np.argmax(mcts_probs1)\n",
        "\n",
        "    else:\n",
        "        neutral_state = game.change_perspective(state, player)\n",
        "        mcts_probs, pos_value, value = mcts2.search(neutral_state)\n",
        "        action = np.argmax(mcts_probs)\n",
        "\n",
        "    state = game.get_next_state(state, action, player)\n",
        "\n",
        "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
        "\n",
        "    if is_terminal:\n",
        "        print(state)\n",
        "        if value == 1:\n",
        "            print(player, \"won\")\n",
        "        else:\n",
        "            print(\"draw\")\n",
        "        break\n",
        "\n",
        "    player = game.get_opponent(player)"
      ],
      "metadata": {
        "id": "0lFk14YwKuFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jucBpEaGttQC"
      },
      "outputs": [],
      "source": [
        "class TicTacToe():\n",
        "    def __init__(self, rows=3, columns=3):\n",
        "      self.action_size = rows * columns\n",
        "      self.columns = columns\n",
        "      self.rows = rows\n",
        "\n",
        "    def __repr__(self):\n",
        "      return \"TicTacToe\"\n",
        "\n",
        "    def get_initial_state(self):\n",
        "      return np.zeros((self.rows, self.columns))\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "      row = action // self.columns\n",
        "      column = action % self.columns\n",
        "      state[row, column] = player\n",
        "      return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "      return ((state.reshape(-1) == 0).astype(np.uint8))\n",
        "\n",
        "    def canonical_boards(self):\n",
        "      pass\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "      if action == None:\n",
        "        return False\n",
        "\n",
        "      row = action // self.columns\n",
        "      column = action % self.columns\n",
        "      player = state[row, column]\n",
        "\n",
        "      return (\n",
        "          np.sum(state[row, :]) == player * self.columns\n",
        "          or np.sum(state[:, column]) == player * self.rows\n",
        "          or np.sum(np.diag(state)) == player * self.rows\n",
        "          or np.sum(np.diag(np.flip(state, axis=0))) == player * self.rows\n",
        "      )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "      return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "      return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      if len(state.shape) == 3:\n",
        "        encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "      return encoded_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZero:\n",
        "  def __init__(self, model, optimizer, policy_loss_fn, value_loss_fn, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.policy_loss_fn = policy_loss_fn\n",
        "    self.value_loss_fn = value_loss_fn\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.mcts = MCTS(game, args, model)\n",
        "\n",
        "  def selfPlay(self):\n",
        "    memory = []\n",
        "    player = 1\n",
        "    state = self.game.get_initial_state()\n",
        "\n",
        "    while True:\n",
        "       neutral_state = self.game.change_perspective(state, player)\n",
        "       action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "       memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "       temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "       action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
        "\n",
        "       state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "       value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "       if is_terminal:\n",
        "        returnMemory = []\n",
        "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "          returnMemory.append((\n",
        "              self.game.get_encoded_state(hist_neutral_state),\n",
        "              hist_action_probs,\n",
        "              hist_outcome\n",
        "          ))\n",
        "        return returnMemory\n",
        "\n",
        "        player = self.game.get_opponent(player)\n",
        "\n",
        "\n",
        "  def train(self, memory):\n",
        "    random.shuffle(memory)\n",
        "    for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "      sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "      state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "      state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "      state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
        "      policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
        "      value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
        "\n",
        "      out_policy, out_value = self.model(state)\n",
        "      policy_loss = self.policy_loss_fn(out_policy, policy_targets)\n",
        "      value_loss = self.value_loss_fn(out_value, value_targets)\n",
        "      loss = policy_loss + value_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = []\n",
        "\n",
        "      self.model.eval()\n",
        "      for selfPlay_iteration in range(self.args['num_selfPlay_iterations']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "      for epoch in range(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ],
      "metadata": {
        "id": "67MUjQzVHbuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search without parallelization for inference.\n",
        "\n",
        "    Attributes:\n",
        "    - game: An instance of the game class defining game mechanics and rules.\n",
        "    - args: A dictionary of arguments/hyperparameters for training and MCTS.\n",
        "    - model: The neural network model used to predict policy and value outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, game, args, model):\n",
        "        self.game = game\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, state):\n",
        "        root = Node(self.game, self.args, state, visit_count=1)\n",
        "\n",
        "        policy, pos_value = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
        "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
        "\n",
        "        valid_moves = self.game.get_valid_moves(state).reshape(-1)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "        root.expand(policy)\n",
        "\n",
        "        for search in range(self.args['num_mcts_searches']):\n",
        "            node = root\n",
        "\n",
        "            while node.is_fully_expanded():\n",
        "                node = node.select()\n",
        "\n",
        "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "            value = self.game.get_opponent_value(value)\n",
        "\n",
        "            if not is_terminal:\n",
        "                policy, value = self.model(\n",
        "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
        "                )\n",
        "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "                valid_moves = self.game.get_valid_moves(node.state).reshape(-1)\n",
        "                policy *= valid_moves\n",
        "                policy /= np.sum(policy)\n",
        "\n",
        "                value = value.item()\n",
        "\n",
        "                node.expand(policy)\n",
        "\n",
        "            node.backpropagate(value)\n",
        "\n",
        "\n",
        "        action_probs = np.zeros(self.game.action_size)\n",
        "        for child in root.children:\n",
        "            action_probs[child.action_taken] = child.visit_count\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        return action_probs, pos_value.item(), value"
      ],
      "metadata": {
        "id": "cgzy2WSR8xuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}